{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7757309-e8fe-4722-bb9d-47f928fd8da1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"workspace\"\n",
    "SCHEMA  = \"default\"\n",
    "DRY_RUN = False   # <- auf False setzen, um das Löschen auszuführen\n",
    "\n",
    "spark.sql(f\"USE CATALOG `{CATALOG}`\")\n",
    "spark.sql(f\"USE SCHEMA `{SCHEMA}`\")\n",
    "\n",
    "def run(sql):\n",
    "    if DRY_RUN:\n",
    "        print(sql)\n",
    "    else:\n",
    "        spark.sql(sql)\n",
    "\n",
    "def pick_name(row, prefs=(\"name\",\"tableName\",\"viewName\",\"function\",\"functionName\",\"volume\")):\n",
    "    d = row.asDict()\n",
    "    for k in prefs:\n",
    "        if k in d:\n",
    "            return d[k]\n",
    "    return row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61a19703-8cf3-4a44-a56b-71dce3b688ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. (DRY_RUN = False )\n"
     ]
    }
   ],
   "source": [
    "# 1) Views droppen\n",
    "df_views = spark.sql(f\"SHOW VIEWS IN `{CATALOG}`.`{SCHEMA}`\")\n",
    "views = [pick_name(r) for r in df_views.collect()]\n",
    "for v in views:\n",
    "    run(f\"DROP VIEW IF EXISTS `{CATALOG}`.`{SCHEMA}`.`{v}`\")\n",
    "\n",
    "# 2) Tabellen droppen\n",
    "df_tables = spark.sql(f\"SHOW TABLES IN `{CATALOG}`.`{SCHEMA}`\")\n",
    "tables = [pick_name(r) for r in df_tables.collect()]\n",
    "for t in tables:\n",
    "    run(f\"DROP TABLE IF EXISTS `{CATALOG}`.`{SCHEMA}`.`{t}`\")\n",
    "\n",
    "# 3) User-Functions droppen\n",
    "df_funcs = spark.sql(f\"SHOW USER FUNCTIONS IN `{CATALOG}`.`{SCHEMA}`\")\n",
    "funcs = [pick_name(r) for r in df_funcs.collect()]\n",
    "for f in funcs:\n",
    "    run(f\"DROP FUNCTION IF EXISTS `{CATALOG}`.`{SCHEMA}`.`{f}`\")\n",
    "\n",
    "# 4) Volumes droppen (am Ende, wenn nichts mehr referenziert)\n",
    "df_vols = spark.sql(f\"SHOW VOLUMES IN `{CATALOG}`.`{SCHEMA}`\")\n",
    "vols = [pick_name(r) for r in df_vols.collect()]\n",
    "for v in vols:\n",
    "    run(f\"DROP VOLUME IF EXISTS `{CATALOG}`.`{SCHEMA}`.`{v}`\")\n",
    "\n",
    "print(\"Done. (DRY_RUN =\", DRY_RUN, \")\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "70_cleanup_old",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}