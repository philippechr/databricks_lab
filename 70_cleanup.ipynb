{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b340258-cd8e-4e4b-869c-6b1d7e32ddf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83E\uDDF9 STARTE CLEANUP für User: philippe.christen@fhnw.ch\n\uD83D\uDCCD Ziel: workspace.default\n⚠️ Modus: DESTRUCTIVE (Löschen)\n------------------------------------------------------------\n\n--- 1. MLflow Modelle bereinigen ---\n   ⏳ Frage Liste aller Modelle ab (das kann kurz dauern)...\n   \uD83D\uDD0E Habe 0 Modelle zum Löschen gefunden.\n\n--- 2. MLflow Experimente bereinigen ---\n\n--- 3. SQL Objekte (Views, Tabellen, Funktionen) ---\n\n--- 4. Volumes und Dateien ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-11-26 11:10:58.959\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.client.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_MultiThreadedRendezvous\", \"msg\": \"<_MultiThreadedRendezvous of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"Public DBFS root is disabled. Access is denied on path: /tmp/checkpoints\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"Public DBFS root is disabled. Access is denied on path: /tmp/checkpoints\\\", grpc_status:13, created_time:\\\"2025-11-26T11:10:58.959103472+00:00\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_execute_and_fetch_as_iterator\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"1935\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"<frozen _collections_abc>\", \"line\": \"356\"}, {\"class\": null, \"method\": \"send\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"141\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"202\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"174\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"307\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"279\"}, {\"class\": null, \"method\": \"<lambda>\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"175\"}, {\"class\": null, \"method\": \"__iter__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"661\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"543\"}, {\"class\": null, \"method\": \"_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"969\"}]}}\n{\"ts\": \"2025-11-26 11:10:58.959\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.client.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_MultiThreadedRendezvous\", \"msg\": \"<_MultiThreadedRendezvous of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"Public DBFS root is disabled. Access is denied on path: /tmp/checkpoints\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"Public DBFS root is disabled. Access is denied on path: /tmp/checkpoints\\\", grpc_status:13, created_time:\\\"2025-11-26T11:10:58.959103472+00:00\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_execute_and_fetch_as_iterator\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"1935\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"<frozen _collections_abc>\", \"line\": \"356\"}, {\"class\": null, \"method\": \"send\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"141\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"202\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"174\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"307\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"279\"}, {\"class\": null, \"method\": \"<lambda>\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"175\"}, {\"class\": null, \"method\": \"__iter__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"661\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"543\"}, {\"class\": null, \"method\": \"_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"969\"}]}}\n{\"ts\": \"2025-11-26 11:10:58.959\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.client.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_MultiThreadedRendezvous\", \"msg\": \"<_MultiThreadedRendezvous of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"Public DBFS root is disabled. Access is denied on path: /tmp/checkpoints\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"Public DBFS root is disabled. Access is denied on path: /tmp/checkpoints\\\", grpc_status:13, created_time:\\\"2025-11-26T11:10:58.959103472+00:00\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_execute_and_fetch_as_iterator\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"1935\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"<frozen _collections_abc>\", \"line\": \"356\"}, {\"class\": null, \"method\": \"send\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"141\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"202\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"174\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"307\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"279\"}, {\"class\": null, \"method\": \"<lambda>\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py\", \"line\": \"175\"}, {\"class\": null, \"method\": \"__iter__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"661\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"543\"}, {\"class\": null, \"method\": \"_next\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"969\"}]}}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n\uD83C\uDF89 CLEANUP COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "# --- KONFIGURATION ---\n",
    "CATALOG = \"workspace\"\n",
    "SCHEMA  = \"default\"\n",
    "DRY_RUN = False   \n",
    "\n",
    "# Username holen (für Experiment-Pfade)\n",
    "try:\n",
    "    current_user = spark.sql(\"SELECT current_user()\").first()[0]\n",
    "except:\n",
    "    current_user = \"unknown\"\n",
    "\n",
    "print(f\"\uD83E\uDDF9 STARTE CLEANUP für User: {current_user}\")\n",
    "print(f\"\uD83D\uDCCD Ziel: {CATALOG}.{SCHEMA}\")\n",
    "print(f\"⚠️ Modus: {'DRY RUN (nur Simulation)' if DRY_RUN else 'DESTRUCTIVE (Löschen)'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Init\n",
    "spark.sql(f\"USE CATALOG `{CATALOG}`\")\n",
    "spark.sql(f\"USE SCHEMA `{SCHEMA}`\")\n",
    "client = MlflowClient()\n",
    "\n",
    "def run_sql(sql):\n",
    "    if DRY_RUN:\n",
    "        print(f\"[SQL Dry] {sql}\")\n",
    "    else:\n",
    "        try:\n",
    "            spark.sql(sql)\n",
    "            print(f\"✅ {sql}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Fehler bei: {sql} -> {e}\")\n",
    "\n",
    "def pick_name(row, prefs=(\"name\",\"tableName\",\"viewName\",\"function\",\"functionName\",\"volume\")):\n",
    "    d = row.asDict()\n",
    "    for k in prefs:\n",
    "        if k in d: return d[k]\n",
    "    return row[1]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. MLflow MODELLE löschen\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n--- 1. MLflow Modelle bereinigen ---\")\n",
    "try:\n",
    "    print(\"   ⏳ Frage Liste aller Modelle ab (das kann kurz dauern)...\")\n",
    "    # Wir holen einfach die ersten 1000 Modelle ohne Filter (stabiler)\n",
    "    all_models = client.search_registered_models(max_results=1000)\n",
    "    \n",
    "    # Wir filtern selbst in Python, das ist oft schneller als Server-Side\n",
    "    models_to_delete = [m for m in all_models if m.name.startswith(f\"{CATALOG}.{SCHEMA}.\")]\n",
    "    \n",
    "    print(f\"   \uD83D\uDD0E Habe {len(models_to_delete)} Modelle zum Löschen gefunden.\")\n",
    "\n",
    "    for model in models_to_delete:\n",
    "        if DRY_RUN:\n",
    "            print(f\"[MLflow Dry] Würde Modell löschen: {model.name}\")\n",
    "        else:\n",
    "            print(f\"   \uD83D\uDDD1️ Lösche Modell '{model.name}'...\")\n",
    "            try:\n",
    "                # 1. Versionen löschen\n",
    "                versions = client.search_model_versions(f\"name='{model.name}'\")\n",
    "                for v in versions:\n",
    "                    client.delete_model_version(name=model.name, version=v.version)\n",
    "                    print(f\"      - Version {v.version} gelöscht.\")\n",
    "                \n",
    "                # 2. Modell löschen\n",
    "                client.delete_registered_model(name=model.name)\n",
    "                print(f\"      ✅ Modell entfernt.\")\n",
    "            except Exception as e:\n",
    "                print(f\"      ⚠️ Fehler beim Löschen von {model.name}: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ KRITISCHER FEHLER beim Abrufen der Modelle: {e}\")\n",
    "    print(\"   -> Überspringe MLflow-Cleanup und mache mit SQL weiter...\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. MLflow EXPERIMENTE löschen\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n--- 2. MLflow Experimente bereinigen ---\")\n",
    "# Wir suchen Experimente im Home-Ordner des Users, die nach unseren Kurs-Namen klingen\n",
    "experiments = mlflow.search_experiments(filter_string=f\"name LIKE '/Users/{current_user}/%'\")\n",
    "\n",
    "# Liste der bekannten Kurs-Experimente (Sicherheitsfilter, damit wir nicht alles löschen)\n",
    "target_experiments = [\"House_Price_Predictor\", \"Dashboard_Priority_Predictor\", \"Course_Price_Prediction\"]\n",
    "\n",
    "for exp in experiments:\n",
    "    # Prüfen, ob der Experiment-Name einen unserer Kurs-Namen enthält\n",
    "    if any(target in exp.name for target in target_experiments):\n",
    "        if DRY_RUN:\n",
    "            print(f\"[MLflow Dry] Würde Experiment löschen: {exp.name}\")\n",
    "        else:\n",
    "            try:\n",
    "                client.delete_experiment(exp.experiment_id)\n",
    "                print(f\"✅ Experiment '{exp.name}' gelöscht.\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Fehler bei Experiment {exp.name}: {e}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. SQL Objekte (Dein Code - Optimiert)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n--- 3. SQL Objekte (Views, Tabellen, Funktionen) ---\")\n",
    "\n",
    "# Views\n",
    "df_views = spark.sql(f\"SHOW VIEWS IN `{CATALOG}`.`{SCHEMA}`\")\n",
    "for r in df_views.collect():\n",
    "    v = pick_name(r)\n",
    "    run_sql(f\"DROP VIEW IF EXISTS `{CATALOG}`.`{SCHEMA}`.`{v}`\")\n",
    "\n",
    "# Tabellen\n",
    "df_tables = spark.sql(f\"SHOW TABLES IN `{CATALOG}`.`{SCHEMA}`\")\n",
    "for r in df_tables.collect():\n",
    "    t = pick_name(r)\n",
    "    run_sql(f\"DROP TABLE IF EXISTS `{CATALOG}`.`{SCHEMA}`.`{t}`\")\n",
    "\n",
    "# Funktionen\n",
    "df_funcs = spark.sql(f\"SHOW USER FUNCTIONS IN `{CATALOG}`.`{SCHEMA}`\")\n",
    "for r in df_funcs.collect():\n",
    "    f = pick_name(r)\n",
    "    run_sql(f\"DROP FUNCTION IF EXISTS `{CATALOG}`.`{SCHEMA}`.`{f}`\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. VOLUMES & DATEIEN (Der physische Müll)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n--- 4. Volumes und Dateien ---\")\n",
    "\n",
    "# Zuerst den Inhalt der Volumes löschen (Checkpoints, JSONs, CSVs)\n",
    "# Sonst bleibt der Müll auf der Festplatte, auch wenn das Volume-Objekt weg ist.\n",
    "df_vols = spark.sql(f\"SHOW VOLUMES IN `{CATALOG}`.`{SCHEMA}`\")\n",
    "vols = [pick_name(r) for r in df_vols.collect()]\n",
    "\n",
    "for v in vols:\n",
    "    path = f\"/Volumes/{CATALOG}/{SCHEMA}/{v}\"\n",
    "    if DRY_RUN:\n",
    "        print(f\"[FS Dry] Würde Inhalt löschen von: {path}\")\n",
    "        print(f\"[SQL Dry] Würde Volume droppen: {v}\")\n",
    "    else:\n",
    "        try:\n",
    "            # Inhalt löschen (rekursiv)\n",
    "            dbutils.fs.rm(path, True)\n",
    "            print(f\"✅ Dateien in '{path}' gelöscht.\")\n",
    "            # Volume Objekt löschen\n",
    "            spark.sql(f\"DROP VOLUME IF EXISTS `{CATALOG}`.`{SCHEMA}`.`{v}`\")\n",
    "            print(f\"✅ Volume '{v}' gedropped.\")\n",
    "        except Exception as e:\n",
    "             print(f\"⚠️ Fehler bei Volume {v}: {e}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. Checkpoints im DBFS (Falls wir /tmp genutzt haben)\n",
    "# ---------------------------------------------------------\n",
    "if not DRY_RUN:\n",
    "    # Optional: Alte Checkpoints aus früheren Lektionen im DBFS root\n",
    "    try:\n",
    "        dbutils.fs.rm(\"/tmp/checkpoints\", True)\n",
    "        print(\"\\n✅ /tmp/checkpoints bereinigt.\")\n",
    "    except: pass\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"\uD83C\uDF89 CLEANUP COMPLETE!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "70_cleanup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}